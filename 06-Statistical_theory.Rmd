# (PART) Analyses inférentielles {-}

# Prérequis

## Préambule

Réaliser une inférence statistique consiste à conclure à propos de quelque chose dans une population d'intérêt (cette chose pouvant se traduire par n'importe quel paramètre statistique : une moyenne, une médiane, une proportion, un coefficient de corrélation, une différence de moyennes, un rapport de cotes, etc.), cela à partir de données prélevées dans un échantillon de cette population. Pour comprendre la mécanique des calculs à mettre en oeuvre pour réaliser une inférence statistique, il est nécessaire d'avoir quelques notions en matière de probabilité. Ce chapitre, qui s'inspire largement du chapitre "Introduction to probability" de l'ouvrage de Danielle Navarro [-@navarroLearningStatistics2018], vise à présenter brièvement ces notions.

## Lois de probabilité

### Notion de loi de probabilité

Les procédures de calcul pour réaliser une inférence statistique requièrent d'utiliser des lois mathématiques que l'on doit configurer pour déterminer théoriquement les probabilités de rencontrer telle ou telle valeur d'un paramètre statistique donné lorsqu'on étudie un échantillon provenant de la population d'intérêt. Cette démarche implique de comprendre que si un phénomène existe (ou pas) à l'échelle d'une population (chose quantifiable à l'aide de la valeur d'un paramètre statistique donné), l'étude d'un échantillon provenant de cette population ne donnera pas forcément la même valeur pour le paramètre statistique considéré. L'enjeu est alors de pouvoir déterminer la probabilité qui était celle d'obtenir telle ou telle valeur du paramètre statistique considéré avec son échantillon dans le cas où le phénomène étudié existerait (ou pas), cela pour envisager une conclusion quant à la réelle existence ou non du phénomène étudié dans la population d'intérêt.

Ces lois mathématiques utilisées pour faire des inférences statistiques, qu'on appelle aussi des lois de probabilité, donnent directement les valeurs de probabilité d'obtenir telle ou telle valeur du paramètre statistique considéré dans les cas de variables qualitatives. Dans les cas de variables quantitatives, les lois de probabilité ne donnent pas directement les probabilités. En effet, dans ces situations, les lois donnent les **densités de probabilité**. Ceci est lié au fait que dans ce type de situations, les probabilités concernent le fait d'avoir des valeurs appartenant à des intervalles donnés, et non pas d'avoir une valeur précise. Cela implique que pour obtenir la probabilité de rencontrer une valeur appartenant à un intervalle donné, il ne faut pas prendre la valeur directement donnée par la loi de probabilité, mais l'intégrale (i.e., l'aire sous la courbe de densité de probabilité) correspondant à l'intervalle de valeurs considéré. Ce qui suit vise à décrire et illustrer des lois de probabilité typiquement utilisées dans les procédures de calcul servant à réaliser des inférences statistiques.

### La loi binomiale

La loi binomiale est une loi mathématique qui concerne les situations où seulement deux résultats sont possibles, comme par exemple "succès" et "échec", "pile" et "face", "0" et "1", etc. Un exemple classique où l'on est en présence d'une variable binomiale est celui où l'on demande à $N$ personnes de lancer une fois une pièce non truquée, avec par conséquent "pile" et "face" comme seuls résultats possibles et autant de chances de tomber sur "pile" que sur "face" à chaque lancer ($\theta$ = 0.5, soit une chance sur deux). Une fois les essais de toutes les personnes terminés, la distribution qui en résulte contient alors la proportion de personnes qui ont obtenu "pile" et la proportion de personnes qui n'ont pas obtenu "pile" (donc "face"). Lorsque le nombre $N$ d'essais ou de participants et la probabilité d'avoir tel ou tel résultat ($\theta$) sont connus, la loi binomiale, qu'on note $X \sim B(\theta, N)$, permet de connaître la probabilité $P(X)$ d'avoir $X$ succès (e.g., $X$ fois "pile") sur les $N$ essais ou participants. La formule de cette loi est montrée ci-dessous : 


$$P(X | \theta, N) = \frac{N!}{X!(N-X)!}\theta^X(1-\theta)^{N-X}$$



La Figure \@ref(fig:distriBinom) illustre une loi binomiale (avec $N$ = 100 et $\theta$ = 0.5) qui pourrait s'appliquer au cas d'un lancer de pièce non truquée. Cette figure montre que même si une pièce est non truquée ($\theta$ = 0.5), il est tout à fait possible qu'il y ait plus de personnes à obtenir "pile" plutôt que "face" et inversement sur un échantillon de 100 personnes. Cependant, ce que montre aussi la figure, c'est que si la pièce est non truquée, les chances restent les plus fortes pour l'obtention de 50 % de personnes avec "pile" et avec "face" pour un lancer de pièce.

```{r distriBinom, message=F, fig.height = 8, out.width='80%', echo = FALSE, fig.cap= "Illustration d'une loi binomiale avec la situation de 100 personnes lançant une pièce non truquée"}
set.seed(1)
binom <- 
  data.frame(
    x = seq(0, 100, 1)
  ) %>%
  dplyr::mutate(
    prob = dbinom(x, 100, 0.5)
  )

ggplot(data = binom, aes(x = x, y  = prob)) + 
  geom_segment(aes(x = x, xend = x, y = 0, yend = prob)) +
  labs(x = "Nombre de fois où \"pile\" est observé sur 100 personnes : X", y = "Probabilité (% de chances) d'obtenir \n X fois \"pile\" sur 100 personnes : P(X)") +
  theme_bw()
```

Plusieurs fonctions R peuvent être utilisées pour obtenir des informations liées aux probabilités données par une loi binomiale. Ces fonctions sont montrées ci-dessous.

```{r}
# Fonction pour déterminer la probabilité d'obtenir précisément x succès 
# (ici 50) à partir de N essais/personnes (ici 100) et d'une probabilité 
# de succès donnée (ici 0.5)
dbinom(x = 50, size = 100, prob = 0.5)

# Fonction pour déterminer la probabilité d'obtenir un nombre de succès
# inférieur ou égal à q (ici 50) à partir de N essais/personnes (ici 100) 
# et d'une probabilité de succès donnée (ici 0.5)
pbinom(q = 50, size = 100, prob = 0.5)

# Fonction pour déterminer la valeur pour laquelle il y a une probabilité p 
# d'obtenir une valeur inférieure ou égale à la valeur définie à partir de N 
# essais/personnes (ici 100) et d'une probabilité de succès donnée (ici 0.5)
qbinom(p = 0.6, size = 100, prob = 0.5)
```


### La loi normale et lois apparentées

La **loi normale**, ou encore **loi gaussienne**, qu'on note $X \sim N(\mu, \sigma)$, avec $\mu$ la moyenne de la variable, et $\sigma$ l'écart-type de la variable, est une loi qui permet de déterminer les probabilités de rencontrer une valeur dans tel ou tel intervalle selon la formule ci-dessous : 

$$p(X | \mu, \sigma) = \frac{1}{\sqrt{2\pi}\sigma} exp(-\frac{(X-\mu)^2}{2\sigma^2}),$$
$p(X | \mu, \sigma)$ étant la densité de probabilité correspondant à la valeur $X$. Il s'agit bien ici de la densité de probabilité, et non pas de la valeur de la probabilité d'obtenir une valeur précise. Pour mieux comprendre les valeurs que donne cette loi mathématique, regardons la Figure \@ref(fig:distiNorm) (graphique de gauche). Sur cette figure, la courbe noire représente les valeurs données par la fonction définissant la loi normale avec $\mu$ = 1, et $\sigma$ = 1. Ces valeurs en réalité n'ont pas vraiment d'intérêt en soi. Par contre, elle permettent de délimiter une aire (en rouge) entre elles et l'axe horizontal, la valeur de cette aire étant pour le coup la probabilité d'obtenir une valeur incluse dans l'intervalle de valeurs relatif à l'aire sous la courbe considérée. Ainsi, l'aire sous l'ensemble de la courbe représentant la densité de probabilité est associée à une probabilité de 1 (il y a par définition, lors d'un tirage au sort, 100 % de chances de rencontrer une valeur située entre le minimum et le maximum de la variable modélisée à l'aide de cette loi). Dans la même veine, le graphique de droite de la Figure \@ref(fig:distiNorm) montre une aire sous la courbe (en rouge) dont la valeur est la probabilité d'obtenir une valeur comprise en 2 et 3 lorsqu'on tire au sort une observation en provenance de la population représentée par une loi normale de moyenne 1 et d'écart-type 1, la probabilité étant ici de `r round(pnorm(3, 1, 1) - pnorm(2, 1, 1), 2) * 100` %.

```{r distiNorm, out.width='90%', message=FALSE, warning=FALSE, echo = FALSE, fig.cap = "Densité de probabilité d'une loi normale"}
# Creating distributions
set.seed(123)
norm <- 
  data.frame(
    X = rnorm(1000, 1, 1)
  ) %>%
  dplyr::mutate(density = dnorm(X, 1, 1))

g1 <- ggplot(data = norm, aes(x = X)) + 
  geom_line(aes(y = density), color = "black", size = 1) +
  geom_area(aes(x = X, y = density), fill = "red", alpha = 0.2) +
  labs(x = "Valeur observée", y = "Densité de probabilité") +
  theme_bw()

g2 <- ggplot(data = norm, aes(x = X)) + 
  geom_line(aes(y = density), color = "black", size = 1) +
  geom_area(data = norm %>% dplyr::filter(X >=2 & X <=3), aes(x = X, y = density), fill = "red", alpha = 0.2) +
  labs(x = "Valeur observée", y = "Densité de probabilité") +
  theme_bw()

g1 | g2
    
```

Plusieurs fonctions R peuvent être utilisées pour obtenir des informations liées à la distribution d'une loi normale donnée. Ces fonctions sont montrées ci-dessous.

```{r}
# Fonction pour déterminer la densité de probabilité correspondant à la valeur x
dnorm(x = 1, mean = 1, sd = 1)

# Fonction pour déterminer la probabilité d'obtenir une valeur inférieure ou 
# égale à q
pnorm(q = 2, mean = 1, sd = 1)

# Fonction pour déterminer la valeur pour laquelle il y a une probabilité p 
# d'obtenir une valeur inférieure ou égale à la valeur définie
qnorm(p = 0.7, mean = 1, sd = 1)
```

La loi normale peut être mise en lien avec d'autres grandes lois, telles que :

* La loi Chi-carré ($\chi^2$) : lorsque l'on prend les valeurs de plusieurs distributions normales standards (avec des moyennes de 0 et des écarts-types de 1), qu'on les met au carré, puis qu'on les additionne, on obtient une variable suivant une loi $\chi^2$ à $k$ degrés de liberté  (cf. Figure \@ref(fig:VariousQuantLaws), graphique A), $k$ étant le nombre de variables que l'on a mises au carré. Comme on peut le voir sur la Figure \@ref(fig:VariousQuantLaws), la distribution $\chi^2$ est plutôt asymétrique, avec des valeurs toujours supérieures à 0.

* La loi $t$ : les distributions relatives à des lois $t$ ressemblent aux distributions relatives à des lois normales mais avec des queues de distribution plus épaisses (cf. Figure \@ref(fig:VariousQuantLaws), graphique B). Une distribution $t$ peut être obtenue en divisant les valeurs d'une distribution $\chi^2$ par le nombre de degrés de liberté $k$, puis en prenant leurs racines carrées, et enfin en divisant les valeurs d'une loi normale par la variable obtenue. On obtient alors une distribution $t$ à $k$ degrés de liberté (cf. Figure \@ref(fig:VariousQuantLaws), graphique C).
 
* La loi $F$ : la distribution d'une loi $F$ ressemble à celle d'une loi $\chi^2$. Une distribution $F$ sert à comparer deux distributions $\chi^2$.

```{r  VariousQuantLaws, out.width='100%', fig.width=11, fig.height=10,  message=F, warning=F, echo = FALSE, fig.cap="Densité de probabilité de lois chi-carré, t, et F."}
# Creating distributions
dist <- 
  data.frame(
    x = seq(0, 100, 1/100),
    x2 = seq(0, 100, 1/100) - 50
    )  %>% 
  dplyr::mutate(
    chi_1 = dchisq(x, 1),
    chi_3 = dchisq(x, 3),
    chi_5 = dchisq(x, 5),
    chi_7 = dchisq(x, 7),
    chi_9 = dchisq(x, 9),
    t_1 = dt(x2, 1),
    t_3 = dt(x2, 3),
    t_5 = dt(x2, 5),
    t_7 = dt(x2, 7),
    t_9 = dt(x2, 9),
    f_1 = df(x, 1, 20),
    f_3 = df(x, 5, 20),
    f_5 = df(x, 10, 20),
    f_7 = df(x, 20, 20),
    f_9 = df(x, 100, 20)
) %>%
  pivot_longer(
    cols = -c(x, x2),
    names_to = c(".value", "df"),
    values_to = "value",
    names_sep = "_"
  ) %>%
  mutate(
    df = fct_relevel(df, "1", "3", "5", "7", "9")
  )


# Vizualizing distributions
g1 <- 
  ggplot(data = dist, aes(x = x)) + 
  geom_line(
    aes(y = chi, color = df), 
    size = 1,
    ) +
  coord_cartesian(xlim = c(0, 30), ylim = c(0, 0.5)) +
  labs(
    title = "A : Des distributions chi-carré",
    x = "Valeur observée",
    y = "Densité de probabilité",
    color = "Nombre de degrés \nde liberté") +
  theme_bw() +
  theme(
  legend.position = c(0.82, 0.75),
    legend.background = element_rect(color = "black"),
    legend.title = element_text(face = "bold")
    )

g2 <-
  ggplot(data = dist, aes(x = x)) + 
  geom_line(
    aes(y = t, color = df), 
    size = 1,
    ) +
  coord_cartesian(xlim = c(40, 60)) +
  labs(
    title = "B : Des distributions t",
    x = "Valeur observée",
    y = "Densité de probabilité",
    color = "Nombre de degrés \nde liberté"
    ) +
  theme_bw() +
  theme(
  legend.position = c(0.82, 0.75),
  legend.background = element_rect(color = "black"),
  legend.title = element_text(face = "bold")
  )

g3 <-
  # Exemple ci-dessous inspiré de https://www.geo.fu-berlin.de/en/v/soga/Basics-of-statistics/Continous-Random-Variables/F-Distribution/index.html
  ggplot(data = dist %>% mutate(
    df = fct_recode(df, "v1 = 1, v2 = 20" = "1", "v1 = 5, v2 = 20" = "3", "v1 = 10, v2 = 20" = "5", "v1 = 20, v2 = 20" = "7", "v1 = 100, v2 = 20" = "9")
    ), 
    aes(x = x)) + 
  geom_line(
    aes(y = f, color = df), 
    size = 1,
    ) +
  coord_cartesian(xlim = c(0, 5), ylim = c(0, 2)) +
  labs(title = "C : Des distributions F") +
  theme_bw() +
  labs(
    title = "C : Des distributions F",
    x = "Valeur observée",
    y = "Densité de probabilité",
    color = "Nombre de degrés \nde liberté"
    ) +
  theme_bw() +
  theme(
  legend.position = c(0.82, 0.75),
  legend.background = element_rect(color = "black"),
  legend.title = element_text(face = "bold")
  )

layout <- "
AABB
#CC#
"
g1 + g2 + g3 + plot_layout(design = layout)
```

## Loi des grands nombres, distribution d'échantillonnage de la moyenne, et théorème de la limite centrale

La loi des grands nombres décrit un principe de probabilité permettant de comprendre bon nombre de phénomènes statistiques, notamment lorsque l'on s'intéresse à la moyenne d'un échantillon. Cette loi implique par exemple le fait que la moyenne d'un échantillon pris au hasard dans une population tend à être plus proche de la moyenne réelle d'une population à mesure que la taille de l'échantillon étudié est grande. Cette loi est illustrée sur la Figure \@ref(fig:GreatNumLaw). Sur cette figure, les distributions représentent des échantillons crées de manière aléatoire à partir d'une population ayant pour moyenne 0 et pour écart-type 400. Le trait vertical rouge représente la moyenne de la population d'origine alors que le trait en pointillés noirs montre la moyenne de l'échantillon qui a été obtenue. Lorsqu'on regarde chaque colonne de la figure du haut vers le bas et qu'on compare les colonnes entre elles, on se rend compte effectivement que plus la taille $N$ de l'échantillon tiré de la population est grande, plus le trait noir se retrouve en général proche du trait rouge, cela traduisant le fait qu'on a de meilleures chances que la moyenne de l'échantillon étudié soit plus proche de la moyenne de la population lorsque l'échantillon est de grande taille. Une question qui pourrait alors se poser est le nombre de personnes ou d'individus que doit contenir l'échantillon pour obtenir un résultat avec une marge d'erreur jugée acceptable.

```{r GreatNumLaw, message=F, warning=F, echo=FALSE, out.width='100%', fig.width = 13, fig.height = 9, fig.align="center", fig.cap="Illustration de la loi des grands nombres avec la moyenne d'un échantillon. Les distributions ont été obtenues à partir de $N$ valeurs obtenues aléatoirement à partir d'une population de moyenne 0 et d'écart-type 400. Trait rouge = moyenne de la population d'origine ; trait noir = moyenne de l'échantillon"}

set.seed(123)
norm1 <- data.frame(x = rnorm(mean = 0, sd = 400, n = 2))
norm2 <- data.frame(x = rnorm(mean = 0, sd = 400, n = 20))
norm3 <- data.frame(x = rnorm(mean = 0, sd = 400, n = 2000))


set.seed(456)
norm5 <- data.frame(x = rnorm(mean = 0, sd = 400, n = 2))
norm6 <- data.frame(x = rnorm(mean = 0, sd = 400, n = 20))
norm7 <- data.frame(x = rnorm(mean = 0, sd = 400, n = 2000))


set.seed(789)
norm9 <- data.frame(x = rnorm(mean = 0, sd = 400, n = 2))
norm10 <- data.frame(x = rnorm(mean = 0, sd = 400, n = 20))
norm11 <- data.frame(x = rnorm(mean = 0, sd = 400, n = 2000))


g1 <-
  ggplot(data = norm1, aes(x = x)) + 
  geom_histogram(fill = "grey", color = "black", binwidth = 100) + 
  geom_vline(aes(xintercept = 0), color = "red", size = 1) +
  geom_vline(aes(xintercept = mean(x)), color = "black", linetype = "dashed", size = 1) +
  labs(title = "N = 2 / Échantillon 1") +
  coord_cartesian(xlim = c(-900, 900))

g2 <-
  ggplot(data = norm2, aes(x = x)) + 
  geom_histogram(fill = "grey", color = "black", binwidth = 100) +
  geom_vline(aes(xintercept = 0), color = "red", size = 1) +
  geom_vline(aes(xintercept = mean(x)), color = "black", linetype = "dashed", size = 1) +
  labs(title = "N = 20 / Échantillon 1")  +
  coord_cartesian(xlim = c(-1200, 1200))

g3 <-
  ggplot(data = norm3, aes(x = x)) +
  geom_histogram(fill = "grey", color = "black", binwidth = 100) +
  geom_vline(aes(xintercept = 0), color = "red", size = 1) +
  geom_vline(aes(xintercept = mean(x)), color = "black", linetype = "dashed", size = 1) +
  labs(title = "N = 2000 / Échantillon 1")  +
  coord_cartesian(xlim = c(-1200, 1200))



g5 <-
  ggplot(data = norm5, aes(x = x)) + 
  geom_histogram(fill = "grey", color = "black", binwidth = 100) +
  geom_vline(aes(xintercept = 0), color = "red", size = 1) +
  geom_vline(aes(xintercept = mean(x)), color = "black", linetype = "dashed", size = 1) +
  labs(title = "N = 2 / Échantillon 2") +
  coord_cartesian(xlim = c(-900, 900))

g6 <-
  ggplot(data = norm6, aes(x = x)) + 
  geom_histogram(fill = "grey", color = "black", binwidth = 100) +
  geom_vline(aes(xintercept = 0), color = "red", size = 1) +
  geom_vline(aes(xintercept = mean(x)), color = "black", linetype = "dashed", size = 1) +
  labs(title = "N = 20 / Échantillon 2")  +
  coord_cartesian(xlim = c(-1200, 1200))

g7 <-
  ggplot(data = norm7, aes(x = x)) +
  geom_histogram(fill = "grey", color = "black", binwidth = 100) +
  geom_vline(aes(xintercept = 0), color = "red", size = 1) +
  geom_vline(aes(xintercept = mean(x)), color = "black", linetype = "dashed", size = 1) +
  labs(title = "N = 2000 / Échantillon 2")  +
  coord_cartesian(xlim = c(-1200, 1200))



g9 <-
  ggplot(data = norm9, aes(x = x)) + 
  geom_histogram(fill = "grey", color = "black", binwidth = 100) + 
  geom_vline(aes(xintercept = 0), color = "red", size = 1) +
  geom_vline(aes(xintercept = mean(x)), color = "black", linetype = "dashed", size = 1) +
  labs(title = "N = 2 / Échantillon 3") +
  coord_cartesian(xlim = c(-900, 900))

g10 <-
  ggplot(data = norm10, aes(x = x)) + 
  geom_histogram(fill = "grey", color = "black", binwidth = 100) +
  geom_vline(aes(xintercept = 0), color = "red", size = 1) +
  geom_vline(aes(xintercept = mean(x)), color = "black", linetype = "dashed", size = 1) +
  labs(title = "N = 20 / Échantillon 3")  +
  coord_cartesian(xlim = c(-1200, 1200))

g11 <-
  ggplot(data = norm11, aes(x = x)) +
  geom_histogram(fill = "grey", color = "black", binwidth = 100) +
  geom_vline(aes(xintercept = 0), color = "red", size = 1) +
  geom_vline(aes(xintercept = mean(x)), color = "black", linetype = "dashed", size = 1) +
  labs(title = "N = 2000 / Échantillon 3")  +
  coord_cartesian(xlim = c(-1200, 1200))



(g1 | g2 | g3) / 
(g5 | g6 | g7) / 
 (g9 | g10 | g11) 
```

La Figure \@ref(fig:GreatNumLaw) montre certes qu'on a plus de chances d'avoir une moyenne d'échantillon proche de la moyenne de la population avec un grand $N$, mais elle esquisse aussi, avec les traits en pointillés noirs, le fait qu'avec un grand $N$, la variabilité des valeurs que peuvent prendre les moyennes de plusieurs échantillons diminue. Pour s'en assurer, on peut chercher à voir les valeurs de moyennes que l'on obtiendrait si l'on étudait un grand nombre d'échantillons (e.g., 10 000) de même taille, autrement dit la distribution d'échantillonnage de la moyenne pour une valeur de $N$ donnée. La Figure \@ref(fig:CentralLimTheo) donne une vision de ce que serait une telle distribution pour différentes valeurs de $N$ dans ce cas là.

```{r CentralLimTheo, message=F, warning=F, echo=FALSE,  out.width='80%', fig.height = 8, fig.align="center", fig.cap="Illustration du théorème de la limite centrale appliqué à une moyenne d'échantillon. Les distributions des moyennes montrées ici ont été obtenues avec 10 000 échantillons de N observations obtenues aléatoirement à partir d'une population ayant pour moyenne 0 et écart-type 400. Trait rouge = moyenne de la population d'origine"}

sample1 <- replicate(mean(rnorm(mean = 0, sd = 400, n = 2)), n = 10000) %>% as.data.frame()
sample2 <- replicate(mean(rnorm(mean = 0, sd = 400, n = 20)), n = 10000) %>% as.data.frame()
sample3 <- replicate(mean(rnorm(mean = 0, sd = 400, n = 2000)), n = 10000) %>% as.data.frame()

g1 <-
  ggplot(data = sample1, aes(x = .)) + 
  geom_histogram(fill = "grey", color = "black", binwidth = 50) + 
  geom_vline(aes(xintercept = 0), color = "red", size = 1) +
  labs(title = "N = 2") +
  coord_cartesian(xlim = c(-1500, 1500))

g2 <-
  ggplot(data = sample2, aes(x = .)) + 
  geom_histogram(fill = "grey", color = "black", binwidth = 50) +  
  geom_vline(aes(xintercept = 0), color = "red", size = 1) +
  labs(title = "N = 20") +
  coord_cartesian(xlim = c(-1500, 1500))

g3 <-
  ggplot(data = sample3, aes(x = .)) + 
  geom_histogram(fill = "grey", color = "black", binwidth = 50) + 
  geom_vline(aes(xintercept = 0), color = "red", size = 1) +
  labs(title = "N = 2000") +
  coord_cartesian(xlim = c(-1500, 1500))

g1 / g2 / g3 
```

La Figure \@ref(fig:CentralLimTheo) illustre plusieurs principes qui relèvent du **théorème de la limite centrale**, à savoir : la moyenne d'une distribution d'échantillonnage de la moyenne tend à être la même moyenne que celle de la population d'origine ; et l'écart-type de la distribution d'échantillonnage de la moyenne (*SEM*, pour *Standard Error of the Mean* en anglais), devient plus faible à mesure que $N$ grandit. Un troisième principe est illustré sur la Figure \@ref(fig:CentralLimTheo2). Pour réaliser cette figure, la même démarche que pour la Figure \@ref(fig:CentralLimTheo) a été suivie, si ce n'est qu'auparavant, la population d'origine suivait systématiquement une loi normale. Dans le cas de la Figure \@ref(fig:CentralLimTheo2), la population suit à l'origine une loi chi-carré, donc asymétrique. Malgré tout, on voit que dès que $N$ est suffisamment grand, la distribution d'échantillonnage de la moyenne suit une loi normale.

```{r CentralLimTheo2, message=F, echo=FALSE, out.width='80%', fig.height = 8, fig.align="center", fig.cap="Illustration du théorème de la limite centrale appliqué à une moyenne d'échantillon. Les distributions des moyennes montrées ici ont été obtenues avec 10 000 échantillons de N observations obtenues aléatoirement à partir d'une population suivant une loi chi-carré avec 3 degrés de liberté. Trait rouge = moyenne de la population d'origine"}

sample1 <- replicate(mean(rchisq(df = 3, n = 2)), n = 10000) %>% as.data.frame()
sample2 <- replicate(mean(rchisq(df = 3, n = 20)), n = 10000) %>% as.data.frame()
sample3 <- replicate(mean(rchisq(df = 3, n = 2000)), n = 10000) %>% as.data.frame()

g1 <-
  ggplot(data = sample1, aes(x = .)) + 
  geom_histogram(fill = "grey", color = "black", binwidth = 0.1) + 
  geom_vline(aes(xintercept = 3), color = "red", size = 1) +
  labs(title = "N = 2") +
  coord_cartesian(xlim = c(0, 15))

g2 <-
  ggplot(data = sample2, aes(x = .)) + 
  geom_histogram(fill = "grey", color = "black", binwidth = 0.1) + 
  geom_vline(aes(xintercept = 3), color = "red", size = 1) +
  labs(title = "N = 20") +
  coord_cartesian(xlim = c(0, 15))

g3 <-
  ggplot(data = sample3, aes(x = .)) + 
  geom_histogram(fill = "grey", color = "black", binwidth = 0.1) + 
  geom_vline(aes(xintercept = 3), color = "red", size = 1)  +
  labs(title = "N = 2000") +
  coord_cartesian(xlim = c(0, 15))

g1 / g2 / g3 
```
Si l'on formalise les choses d'un point de vue plus mathématique, le théorème de la limite centrale nous dit que si une population a une moyenne $\mu$ et un écart-type $\sigma$, alors la moyenne de la distribution d'échantillonnage a aussi $\mu$ comme moyenne, et l'écart-type de la distribution d'échantillonnage de la moyenne (i.e., l'erreur standard, *SEM*) vaut :

$$SEM = \frac{\sigma}{\sqrt{N}}.$$

La formule montre bien que pour une population présentant un écart-type donné, plus les échantillons étudiés seront de petite taille ($N$), plus grande sera la variabilité des moyennes provenant des différents échantillons. D'un point de vue plus pratique, cela veut dire notamment que dans une méta-analyse où l'on chercherait à estimer par exemple la moyenne de l'effet d'une intervention dans une population donnée, on aura beau avoir plein d'études conduites sur le sujet, si elles sont de trop petites tailles, il y a de bonnes chances pour que les valeurs des effets trouvés divergent substantiellement et ne permettent donc pas d'avoir une vue précise, fiable, de la valeur de l'effet concernant la population globale.


# Tests pour des variables qualitatives

## Test du $\chi^2$ d'adéquation (ou encore dit de conformité ou d'ajustement)

Le test du $\chi^2$ d'adéquation consiste à comparer la distribution d'une variable qualitative observée avec une distribution théorique. Pour mieux voir en quoi cela consiste, prenons un exemple repris de Danielle Navarro [-@navarroLearningStatistics2018] où l'on chercherait à savoir si, lorsqu'on demande à des personnes de choisir "au hasard" mentalement une carte parmi un ensemble de cartes étalées devant soi, le choix se fait vraiment de manière entièrement aléatoire. Pour étudier cela, on demande à 200 personnes de réaliser l'expérimentation et on note le type de carte qui a été retenu : coeur, trèfle, carreau, ou pique. Le jeu de données est crée avec le code ci-dessous :

```{r}
# Creation du jeu de données
trefles <- rep("trèfles", 35)
carreau <- rep("carreau", 51)
coeur <- rep("coeur", 64)
pique <- rep("pique", 50)

cartes <- data.frame(id = seq(1, 200, 1), choix = c(trèfles, carreau, coeur, pique))
```

Dans ce cadre, l'idée que les personnes choisiraient leur carte purement au hasard (idée ou hypothèse qu'on va noter $H_{0}$) se traduit par le fait que chaque type de carte aurait la même probabilité d'être tiré à chaque fois, soit une chance sur quatre (0.25). Le vecteur qui résumerait les probabilités $P$ de voir chaque type de carte à chaque tirage serait alors le suivant :

$$H_{0} : P = (0.25, 0.25, 0.25, 0.25).$$

Étant donné qu'il y a 200 personnes ($N$) dans notre exemple, la distribution théorique $E$ des fréquences de tirage de chaque type de carte qui correspondrait à l'hypothèse $H_{0}$ serait tel que $E = N \cdot P$, soit : 

$$E = (50, 50, 50, 50)$$.

On peut aussi mettre cela sous forme de vecteur avec R :

```{r}
expected <- c("carreau" = 50, "coeur" = 50, "pique" = 50, "trefles" = 50)
expected
```


Maintenant que l'on connaît la distribution théorique correspondant à notre exemple, voyons qu'elle est la distribution réelle obtenue suite à notre expérimentation. Pour l'obtenir, on peut résumer la variable `choix` du jeu de données `cartes` crée juste précédemment, cela grâce à la fonction `table()` : 

```{r}
observed <- table(cartes$choix)
observed
```

Cette distribution des observations pourrait s'écrire comme suit : 
$$O = (51, 64, 50, 35)$$

À présent, il convient de calculer une statistique qui résumerait l'écart qu'il peut y avoir entre la distribution théorique, et celle observée. Cette statistique, c'est $X^2$ ou $GOF$ (pour *Goodness Of Fit*), tel que :

$$X^2 = \sum_{i=1}^{k} \frac{(O_{i} - E_{i})^2}{E_{i}},$$

$k$ étant le nombre total de modalités de la variable (ici les types de cartes), $O_{i}$ désignant la fréquence pour $i$-ème modalité de la variable observée, et $E_{i}$ désignant la fréquence pour la $i$-ème modalité de la variable théorique. On peut calculer $X^2$ manuellement : 

```{r}
sum((observed - expected)^2 / expected)
```

Nous avons à présent le score qui résume l'écart obtenu entre la distribution observée et la distribution théorique de notre variable qualitative. Il nous reste alors à connaître la distribution d'échantillonnage de la statistique $X^2$ dans le cas où $H_{0}$ serait vraie. Cela nous permettra de connaître la probabilité de rencontrer une valeur de $X^2$ au moins aussi grande que 8.44 dans le cas où $H_{0}$ serait vraie, et de juger ainsi si cette probabilité supporte ou non $H_{0}$... Trêve de suspens, la distribution d'échantillonnage de $X^2$ suit une loi $\chi^2$ à $k-1$ degrés de liberté ($k$ étant le nombre de types d'évènement possibles, ici les types de cartes tirées). Comment peut-on expliquer cela? Tout d'abord, reprenons le calcul de $X^2$, et plus précisément le terme $O_{i}$. Admettons qu'il ne concerne qu'un seul type de cartes (e.g., les coeurs). Ce terme a sa propre distribution de probabilité en posant que $H_{0}$ soit vraie, et cette distribution est celle d'une loi binomiale (avec $\theta$ = 0.25 et $N$ = 200 dans notre exemple), puisque pour chaque sujet interrogés, il était possible que ce type de carte soit choisi, ou non. Or, il s'avère que lorsque $N$ est relativement grand et que $\theta$ n'est ni trop proche de 0, ni trop proche de 1, la distribution se rapproche d'un loi normale [@navarroLearningStatistics2018]. En allant vite, pour un type de cartes donné, on peut alors considérer que l'expression $\frac{(O_{i} - E_{i})}{E_{i}}$ a une distribution d'échantillonnage qui s'apparente à une loi normale standard, et c'est le cas à chaque fois que cette expression doit être reprise pour chaque type de carte. Si on met au carré chacune de ces variables, puis qu'on les additionne (comme décrit plus haut pour le calcul de $X^2$), on se retrouve dans la situation présentée dans le chapitre précédent où nous obtenons une loi $\chi^2$. La subtilité ici, par rapport au chapitre précédent, est que le nombre de degrés de liberté n'est pas $k$ mais $k-1$. Le nombre de degrés de liberté désigne le nombre de quantités distinctes à décrire (ici 4 quantités car les données sont regroupées en 4 modalités) moins le nombre de contraintes (ici 1 contrainte liée au groupe de sujet qui a un nombre fini). On a donc bien 3 degrés de liberté ici car il nous suffit de connaître 3 scores de chaque modalité pour connaître le 4ème qu'il nous manquerait pour une taille d'échantillon donnée... 

La Figure \@ref(fig:GOFDistri) montre la loi $\chi^2$ relative à 3 degrés de liberté et donc les probabilités de rencontrer tel ou tel intervalle de valeurs de $X^2$ dans le cas où $H_{0}$ serait vraie. La zone hachurée représente la probabilité qui était d'obtenir une valeur de $X^2$ au moins aussi grande que 8.44 dans l'hypothèse où $H_{0}$ serait vraie. Cette probabilité $P$ est de `r, round(pchisq(q = 8.44, df = 3, lower.tail = FALSE), 2) * 100` %. En principe, lors d'un test statistique, il y a aussi ce qu'on appelle une "région critique", c'est-à-dire l'intervalle dans lequel la valeur de $P$ liée à notre valeur de 8.44 devrait se trouver pour qu'on considère que nos résultats ne supportent pas $H_{0}$ et donc qu'on ne retient pas cette hypothèse. Classiquement (voire par défaut, à tort), cette région critique est définie à l'aide du seuil $P <= 0.05$. Autrement dit, il faudrait que notre statistique $X^2$ tombe dans l'intervalle pour lequel il y avait une probabilité inférieure ou égale à 5% de tomber dans le cas où $H_{0}$ serait vraie. Dans notre exemple, cela se traduirait par le fait d'obtenir une valeur de $X^2$ supérieure ou égale à `r qchisq(0.95, 3)`. La région critique correspondant à cette zone est montrée en rouge sur la Figure \@ref(fig:GOFDistri). En suivant cette démarche, $H_{0}$ serait donc bien rejetée puisque notre valeur $X^2$ de 8.44 tomberait dans la région critique. Le test serait alors dit "significatif".

```{r GOFDistri, message=F, echo=FALSE, out.width='80%',  fig.align="center", fig.cap="Distribution d'échantillonnage de la statistique $\\chi^2$ sous $H{0}$"}
data_chisq <- data.frame(x = seq(0, 10, 0.05), y = dchisq(seq(0, 10, 0.05), 3))

library(ggpattern)
ggplot(data = data_chisq, aes(x = x, y = y)) +
  geom_line() +
  geom_area(data = data_chisq %>% filter(x >= qchisq(1-0.05, 3) & x <= 8.47), fill = "red", alpha = 0.5) +
  geom_area(data = data_chisq %>% filter(x >= 8.44), fill = "grey", alpha = 0.5) +
  geom_area_pattern(data = data_chisq %>% filter(x >= 8.44), 
     pattern = 'stripe',
     pattern_density = 0.01,
     fill = "red", alpha = 0.5,
     colour  = 'black',
     pattern_colour = "grey30",
     pattern_spacing = 0.02
  ) +
  annotate("point", x = 8.44, y = 0, size = 3, color = "black") +
  annotate("text", x = 9, y = 0.05, size = 5, label = "8.44", color = "black", fontface = "bold") +
  annotate(geom = "curve", 
             x = 9, 
             y = 0.04, 
             xend = 8.55, 
             yend = 0.004, 
             size = 0.8, 
             color  = "black",
             curvature = -.35, arrow = arrow(length = unit(2, "mm"))
             ) +
  geom_segment(x = 8.44, xend = 10, y = 0, yend = 0, size = 0.4) + 
  geom_segment(x = 8.44, xend = 8.44, y = 0, yend = dchisq(8.44, 3), size = 0.4) + 
    labs(
    x = bquote(Statistisque ~X^2),
    y = "Densité de probabilité"
    )
```

Nous venons de voir la mécanique des calculs qu'il y a derrière le test $\chi^2$ d'adéquation. Heureusement, il existe des fonctions dans R pour faire cela automatiquement, comme la fonction `chisq.test()` : 

```{r}
observed <- table(cartes$choix)
expected <- c(0.25, 0.25, 0.25, 0.25)
chisq.test(x = observed, p = expected)
```

Une seconde fonction possible est la fonction `goodnessOfFitTest()` du package `lsr`. Elle est intéressante en raison du fait qu'elle propose un récapitulatif plus détaillé de la configuration du test qui a été faite. De plus, pas besoin de passer par la fonction `table()`, mais il faut que la variable étudiée soit en format `factor` :

```{r}
lsr::goodnessOfFitTest(as.factor(cartes$choix), p = c(0.25, 0.25, 0.25, 0.25))
```

Pour reporter les résultats, nous pourrions écrire les choses comme cela [@navarroLearningStatistics2018] : Le résultat du test était significatif ($\chi^2(3)$ = 8.44, $P$ = 0.04).

Il convient de noter que les probabilités espérées (théoriques) peuvent être définies à volonté selon la question de recherche. Par exemple, on aurait pu tester l'hypothèse $H_{0}$ selon laquelle les personnes préfèrent à 80 % les coeurs, 10 % les trèfles, 5 % les piques, et 5 % les carreaux. Le tout est de bien configurer le test pour que les fréquences observées pour chaque modalité (ici les types de cartes) soient bien mises en correspondance avec les fréquences théoriques (cf. argument `p`dans l'exemple de code ci-dessous). La fonction `lsr::goodnessOfFitTest()` a donc un fort intérêt ici pour bien vérifier, dans les résultats affichés, qu'on a bien attribué les probabilités théoriques aux bonnes modalités) :

```{r}
lsr::goodnessOfFitTest(as.factor(cartes$choix), p = c(0.05, 0.8, 0.05, 0.1))
```

## Résumé
* Les lois de probabilité, la loi des grands nombres, la distribution d'échantillonnage d'une statistique, ou encore le théorème de la limite centrale, sont des outils mathématiques qui permettent de réaliser et de mieux comprendre les inférences statistiques.
* Une loi de probabilité renseigne sur les chances, pour une statistique donnée, d'obtenir une valeur précise ou d'obtenir une valeur tombant dans un rang de valeurs donné selon le cas de figure.
* Des exemples de lois de probabilité courrement utilisées sont la loi binomiale, la loi normale, la loi $\chi^2$, la loi $t$, et la loi $F$.

